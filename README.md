# Hybrid Parallel Task Scheduler: MPI + OpenMP + SIMD

A high-performance distributed task execution system demonstrating **three levels of parallelism**: distributed memory (MPI), shared memory (OpenMP), and instruction-level vectorization (SIMD).

## ğŸ¯ Overview

This project implements a distributed task scheduler with three parallel variants:
1. **MPI-only** (`distr-sched`) - Baseline distributed memory implementation
2. **MPI+OpenMP** (`distr-sched-openmp`) - Hybrid distributed + shared memory parallelism
3. **MPI+OpenMP+SIMD** (`distr-sched-simd`) - Full three-level parallel optimization

### Course Concepts Demonstrated

| Unit | Concept | Implementation |
|------|---------|----------------|
| **Unit I** | ILP, Pipelining | SIMD vectorization with `#pragma omp simd` |
| **Unit II** | Thread-level parallelism | OpenMP threading in worker nodes |
| **Unit III** | Data-level parallelism | Vector operations on matrix and sort kernels |
| **Unit IV** | Message passing | MPI master-worker with non-blocking communication |

The system employs a master-worker architecture with dynamic load balancing across heterogeneous compute nodes.

## ğŸ—ï¸ Architecture

### Master-Worker Pattern

```text
                  â”Œâ”€â”€â”€ Worker 1 â”€â”€â–º Execute Task
                  â”‚
Master Queue â”€â”€â”€â”€â”€â”¼â”€â”€â”€ Worker 2 â”€â”€â–º Execute Task
                  â”‚
                  â””â”€â”€â”€ Worker N â”€â”€â–º Execute Task
```

### Key Components

1. **Master Node (Rank 0)**

   - Task queue management
   - Worker availability tracking
   - Non-blocking communication handling

2. **Worker Nodes**
   - Task execution
   - Result reporting
   - Dynamic task handling

## ğŸ’» Technical Implementation

### Master Node Management

```cpp
// Master node data structures
queue<task_t> task_queue = get_initial_tasks(params);
vector<bool> availability(N_WORKERS, true);
vector<MPI_Request> proc_requests(N_WORKERS, MPI_REQUEST_NULL);
vector<task_t*> desc_tasks(num_procs - 1, nullptr);

// Non-blocking receive for descendant tasks
MPI_Irecv(desc_tasks[proc], Nmax, TASK_T_TYPE,
          worker_rank, 0, MPI_COMM_WORLD,
          &proc_requests[proc]);
```

### Key Implementations

**OpenMP Threading** (worker nodes):
```cpp
#ifdef _OPENMP
    omp_set_num_threads(2); // Balance MPI and OpenMP
#endif
```

**SIMD Vectorization** (compute kernels):
```cpp
#pragma omp simd
for (int j = 0; j < p; j++) {
    C[{i, j}] += a_ik * B[{k, j}];
}
```

**Non-blocking MPI Communication**:
```cpp
MPI_Waitsome(unavailable_requests.size(),
             unavailable_requests.data(),
             &outcount, indices.data(),
             MPI_STATUSES_IGNORE);
```

## ğŸš€ Quick Start

### Build All Variants
```bash
make clean && make all
```

### For Local Execution (No SLURM)

**Interactive Demo** (â­ Best for presentations):
```bash
./scripts/demo_presentation.sh
```

**With Visualization**:
```bash
./scripts/demo_visualization.sh
```

**Single Test**:
```bash
./scripts/run_local.sh lala 4
```

**Full Benchmark**:
```bash
./scripts/run_benchmark_local.sh
python3 scripts/analyze_performance.py
```

### For SLURM Clusters

```bash
# Single test
sbatch slurm/config1.sh 16 1 2 0.10 tests/tinkywinky.in

# With OpenMP
VARIANT=openmp sbatch slurm/config1.sh 16 1 2 0.10 tests/tinkywinky.in

# Full benchmark suite
./slurm/benchmark.sh
python3 scripts/analyze_performance.py
```

## ğŸ“Š Expected Performance

| Variant | Speedup over MPI-only | Best For |
|---------|----------------------|----------|
| MPI+OpenMP | 1.2-1.5x | Multi-core nodes |
| MPI+OpenMP+SIMD | 1.5-2.0x | Compute-intensive tasks |

### Performance Factors
- **SIMD gains**: Most effective on matrix operations and sorting
- **OpenMP benefits**: Better utilization of multi-core nodes
- **Load balancing**: Dynamic task distribution handles heterogeneous workloads

## ğŸ”§ Project Structure

```
DTSLBF-MPI/
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ main.cpp          # Entry point
â”‚   â”œâ”€â”€ runner.cpp        # MPI+OpenMP coordination logic
â”‚   â”œâ”€â”€ runner_seq.cpp    # Sequential reference
â”‚   â”œâ”€â”€ tasks.cpp         # SIMD-optimized compute kernels
â”‚   â”œâ”€â”€ runner.hpp        # Header files
â”‚   â”œâ”€â”€ tasks.hpp
â”‚   â””â”€â”€ check.py          # Output validation
â”œâ”€â”€ scripts/              # Execution scripts
â”‚   â”œâ”€â”€ demo_presentation.sh       # Interactive demo
â”‚   â”œâ”€â”€ demo_visualization.sh      # Visualization demo
â”‚   â”œâ”€â”€ run_local.sh              # Single test runner
â”‚   â”œâ”€â”€ run_benchmark_local.sh    # Full benchmark suite
â”‚   â”œâ”€â”€ visualize_performance.py  # Performance charts
â”‚   â”œâ”€â”€ visualize_tasks.py        # Task distribution viz
â”‚   â””â”€â”€ analyze_performance.py    # Results analysis
â”œâ”€â”€ slurm/                # SLURM cluster scripts
â”‚   â”œâ”€â”€ config*.sh        # Cluster configurations
â”‚   â”œâ”€â”€ job.sh            # Job execution script
â”‚   â”œâ”€â”€ benchmark.sh      # Automated testing
â”‚   â””â”€â”€ example.sh        # Usage examples
â”œâ”€â”€ docs/                 # Documentation
â”‚   â”œâ”€â”€ PROJECT_EXPLANATION.md    # Complete guide
â”‚   â”œâ”€â”€ PRESENTATION_GUIDE.md     # Jury presentation
â”‚   â”œâ”€â”€ VISUALIZATION_GUIDE.md    # Visualization usage
â”‚   â”œâ”€â”€ RUNNING_LOCALLY.md        # Local execution
â”‚   â””â”€â”€ QUICK_REFERENCE.txt       # Cheat sheet
â”œâ”€â”€ tests/                # Test input files
â”œâ”€â”€ logs/                 # Output logs
â”œâ”€â”€ Makefile              # Build system
â””â”€â”€ README.md             # This file
```

## ğŸ“ Prerequisites

- MPI compiler (`mpic++`) with C++20 support
- OpenMP support (`-fopenmp`)
- CPU with SIMD extensions (AVX2 or better)
- SLURM workload manager (for cluster deployment)

## ğŸ“Š Test Configurations

```bash
# Configuration 1
sbatch config1.sh 16 1 2 0.10 tests/tinkywinky.in
sbatch config1.sh 5 3 5 0.50 tests/dipsy.in
sbatch config1.sh 5 2 2 0.00 tests/lala.in
sbatch config1.sh 5 2 4 0.50 tests/po.in
sbatch config1.sh 12 0 10 0.16 tests/thesun.in
```

## ğŸ“š Test Cases

- `tinkywinky.in` - Large task depth (H=16)
- `dipsy.in` - High generation probability (P=0.50)
- `lala.in` - No task generation (P=0.00)
- `po.in` - Balanced workload
- `thesun.in` - Many initial tasks

## ğŸ† Key Features

### Parallel Programming Concepts
- âœ… **Three-level parallelism hierarchy** (MPI â†’ OpenMP â†’ SIMD)
- âœ… **Distributed memory parallelism** - Master-worker pattern with MPI
- âœ… **Shared memory parallelism** - OpenMP threads within nodes
- âœ… **Instruction-level parallelism** - SIMD vectorization of compute kernels

### Implementation Highlights
- âœ… **Dynamic load balancing** across heterogeneous nodes
- âœ… **Non-blocking communication** (MPI_Isend, MPI_Irecv, MPI_Waitsome)
- âœ… **SIMD-optimized kernels** (matrix multiplication, bitonic sort)
- âœ… **Cache-friendly loop ordering** (ikj instead of ijk)
- âœ… **Automated benchmarking suite** with performance analysis
- âœ… **Comprehensive documentation** and testing framework

### Academic Value
- Demonstrates concepts from Units I-IV of parallel programming
- Quantitative performance comparison across paradigms
- Real-world hybrid parallelism implementation
- Suitable for mini project demonstration

## ğŸ“„ License

This project was developed as part of a distributed systems and parallel programming coursework.
