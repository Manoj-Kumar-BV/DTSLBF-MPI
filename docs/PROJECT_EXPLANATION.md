# Complete Project Explanation: Hybrid Parallel Task Scheduler

## ğŸ“š Table of Contents
1. [Background & Motivation](#background--motivation)
2. [Real-World Scenario](#real-world-scenario)
3. [Core Concepts](#core-concepts)
4. [How the System Works](#how-the-system-works)
5. [The Three Levels of Parallelism](#the-three-levels-of-parallelism)
6. [Code Walkthrough](#code-walkthrough)
7. [Performance Analysis](#performance-analysis)

---

## ğŸ¯ Background & Motivation

### The Problem: CPU Performance Limits

**Historical Context:**
- In the early 2000s, CPU clock speeds were increasing rapidly (Moore's Law)
- Around 2005, we hit a "power wall" - CPUs couldn't get much faster without overheating
- Solution: Instead of making ONE processor faster, use MULTIPLE processors working together

**Modern Computing Reality:**
- Your laptop has multiple cores (4, 8, or more)
- Data centers have thousands of machines
- Each core has special vector units (SIMD) that can process multiple data items at once

**The Challenge:**
How do we make programs that can use ALL these resources efficiently?

### Why Three Levels?

Think of a construction company building multiple houses:

1. **MPI (Distributed Memory)**: Different construction sites (separate machines)
   - Each site has its own workers and materials
   - Sites communicate by phone/email (message passing)
   - Can build in different cities simultaneously

2. **OpenMP (Shared Memory)**: Multiple workers at ONE construction site
   - Workers share the same materials and tools
   - Easy to coordinate (they can see each other)
   - Limited by the size of one site

3. **SIMD (Instruction-Level)**: Using power tools instead of hand tools
   - One worker with a nail gun can do the work of 4 workers with hammers
   - Same person, same time, but processing 4 nails at once

---

## ğŸŒ Real-World Scenario

### Imagine: A Video Streaming Platform (like Netflix)

**The Task Queue:**
- Users submit requests: transcode videos, generate thumbnails, create previews
- Each task can spawn more tasks (e.g., "transcode video" â†’ "encode at 1080p", "encode at 720p", "encode at 480p")
- Tasks have dependencies (can't create thumbnail until video is decoded)

**The Challenge:**
- Thousands of videos to process
- Different videos take different amounts of time
- Multiple servers available (heterogeneous: some fast, some slow)
- Need to maximize throughput while minimizing cost

**Our Solution:**
```
Master Server (Rank 0):
  â”œâ”€ Maintains queue of all pending video processing tasks
  â”œâ”€ Tracks which worker servers are available
  â””â”€ Distributes tasks dynamically

Worker Servers (Ranks 1, 2, 3, ...):
  â”œâ”€ Each server has multiple CPU cores (OpenMP)
  â”œâ”€ Each core has SIMD units for fast processing
  â”œâ”€ Execute tasks: transcode, thumbnail, metadata extraction
  â””â”€ Report back when done (may spawn more subtasks)
```

### Concrete Example Flow:

```
Initial Queue: [Transcode_MovieA, Transcode_MovieB, Transcode_MovieC]

Time 0:
  Master: "I have 3 tasks and 3 idle workers"
  â†’ Send Transcode_MovieA to Worker1
  â†’ Send Transcode_MovieB to Worker2
  â†’ Send Transcode_MovieC to Worker3

Time 5s:
  Worker2 (fastest): "Done with MovieB! Generated 3 subtasks: [720p, 480p, 360p]"
  Master: "Great! Here's the 720p task"
  â†’ Send 720p_MovieB to Worker2

Time 8s:
  Worker1: "Done with MovieA! Generated 2 subtasks: [720p, 480p]"
  Master: "Worker2 is busy, here's another task"
  â†’ Send 720p_MovieA to Worker1

Time 12s:
  Worker3 (slowest): "Finally done with MovieC!"
  Master: "Here's the next pending task"
  â†’ Send 480p_MovieB to Worker3

... and so on until all tasks complete
```

This is **dynamic load balancing** - fast workers get more work automatically!

---

## ğŸ§  Core Concepts

### 1. Parallelism vs Concurrency

**Parallelism:** Actually doing multiple things AT THE SAME TIME
- Requires multiple processors/cores
- Example: 4 chefs each cooking a different dish simultaneously

**Concurrency:** Managing multiple things, but maybe not simultaneously
- Can work on single processor (context switching)
- Example: 1 chef switching between 4 dishes

**This project uses TRUE PARALLELISM** - multiple cores/machines working simultaneously.

### 2. Distributed vs Shared Memory

**Shared Memory (OpenMP):**
```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   RAM (Shared)  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
         â”‚   CPU   â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
  Core1    Core2    Core3
  
  All cores can access the same memory
  Fast communication, but limited scalability
```

**Distributed Memory (MPI):**
```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Machine1 â”‚       â”‚ Machine2 â”‚       â”‚ Machine3 â”‚
  â”‚ CPU+RAM  â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚ CPU+RAM  â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚ CPU+RAM  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                  â–²                   â–²
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Network (Message Passing)
  
  Each machine has its own memory
  Must explicitly send/receive messages
  Scales to thousands of machines
```

**This project uses BOTH** - MPI across machines, OpenMP within each machine!

### 3. SIMD (Single Instruction, Multiple Data)

Traditional Processing:
```
for (i = 0; i < 4; i++) {
    C[i] = A[i] + B[i];
}

Executes as:
  C[0] = A[0] + B[0]   â† 1st cycle
  C[1] = A[1] + B[1]   â† 2nd cycle
  C[2] = A[2] + B[2]   â† 3rd cycle
  C[3] = A[3] + B[3]   â† 4th cycle
  Total: 4 cycles
```

SIMD Processing:
```
#pragma omp simd
for (i = 0; i < 4; i++) {
    C[i] = A[i] + B[i];
}

Executes as:
  C[0,1,2,3] = A[0,1,2,3] + B[0,1,2,3]   â† 1 cycle!
  Total: 1 cycle (4x faster!)
```

Modern CPUs have 256-bit or 512-bit SIMD registers:
- 256-bit = process 8 floats simultaneously (AVX2)
- 512-bit = process 16 floats simultaneously (AVX-512)

---

## ğŸ”§ How the System Works

### System Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   MASTER (Rank 0)       â”‚
                    â”‚                         â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚   Task Queue      â”‚  â”‚
                    â”‚  â”‚  â”Œâ”€â” â”Œâ”€â” â”Œâ”€â” â”Œâ”€â” â”‚  â”‚
                    â”‚  â”‚  â”‚Tâ”‚ â”‚Tâ”‚ â”‚Tâ”‚ â”‚Tâ”‚ â”‚  â”‚
                    â”‚  â”‚  â””â”€â”˜ â””â”€â”˜ â””â”€â”˜ â””â”€â”˜ â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â”‚                         â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ Worker Tracking   â”‚  â”‚
                    â”‚  â”‚ W1: BUSY          â”‚  â”‚
                    â”‚  â”‚ W2: IDLE âœ“        â”‚  â”‚
                    â”‚  â”‚ W3: BUSY          â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚              â”‚
              â–¼              â–¼              â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  WORKER 1    â”‚ â”‚  WORKER 2    â”‚ â”‚  WORKER 3    â”‚
      â”‚  (Rank 1)    â”‚ â”‚  (Rank 2)    â”‚ â”‚  (Rank 3)    â”‚
      â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
      â”‚  OpenMP      â”‚ â”‚  OpenMP      â”‚ â”‚  OpenMP      â”‚
      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚  â”‚Thread 1 â”‚ â”‚ â”‚  â”‚Thread 1 â”‚ â”‚ â”‚  â”‚Thread 1 â”‚ â”‚
      â”‚  â”‚Thread 2 â”‚ â”‚ â”‚  â”‚Thread 2 â”‚ â”‚ â”‚  â”‚Thread 2 â”‚ â”‚
      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
      â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
      â”‚  Each thread â”‚ â”‚  Each thread â”‚ â”‚  Each thread â”‚
      â”‚  uses SIMD   â”‚ â”‚  uses SIMD   â”‚ â”‚  uses SIMD   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Task Structure

Every task has:
```cpp
struct task_t {
    uint32_t id;              // Unique identifier
    int gen;                  // Generation (depth in tree)
    TaskType type;            // PRIME, MATMULT, LCS, SHA, or BITONIC
    uint32_t arg_seed;        // Random seed for task parameters
    uint32_t output;          // Result of computation
    int num_dependencies;     // How many child tasks spawned
    array<uint32_t, 4> dependencies;  // IDs of child tasks
    array<uint32_t, 4> masks; // Dependency information
};
```

### Task Types (The "Work" Being Done)

1. **PRIME**: Check if a large number is prime
   - Simulates: Cryptographic operations
   - Computation: Trial division up to sqrt(n)

2. **MATMULT**: Matrix multiplication
   - Simulates: Graphics rendering, ML training
   - Computation: Multiply two NxN matrices
   - **SIMD Optimized**: Inner loop vectorized

3. **LCS**: Longest Common Subsequence
   - Simulates: DNA sequence alignment, diff tools
   - Computation: Dynamic programming algorithm

4. **SHA**: SHA-256 hash computation
   - Simulates: Blockchain mining, data integrity checks
   - Computation: Cryptographic hash function

5. **BITONIC**: Bitonic sort
   - Simulates: Database indexing, search algorithms
   - Computation: Parallel sorting algorithm
   - **SIMD Optimized**: Comparison operations vectorized

### Task Generation (Creating Work Dynamically)

Each task can spawn 0-N child tasks based on:
- **H** (max depth): Maximum generation level
- **Nmin, Nmax**: Range of children (e.g., 2-4 children per task)
- **P** (probability): Chance of spawning children

Example with H=3, Nmin=2, Nmax=3, P=0.50:
```
Generation 0:  [Task A]
                  â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
           â”‚      â”‚      â”‚
Gen 1:   [B]    [C]    [D]    â† Task A spawned 3 children
           â”‚             â”‚
        â”Œâ”€â”€â”´â”€â”€â”       â”Œâ”€â”€â”´â”€â”€â”
Gen 2: [E]   [F]     [G]   [H]  â† Some spawned 2 children
                      â”‚
                      â””â”€ [I]     â† One spawned 1 child
Gen 3: No more children (hit max depth H=3)
```

---

## ğŸš€ The Three Levels of Parallelism

### Level 1: MPI (Process-Level Parallelism)

**What it does:**
- Launches multiple independent processes (can be on different machines)
- Each process has its own memory space
- Processes communicate by sending messages

**In our code:**
```cpp
// Initialize MPI
MPI_Init(&argc, &argv);
MPI_Comm_rank(MPI_COMM_WORLD, &rank);  // What's my ID?
MPI_Comm_size(MPI_COMM_WORLD, &num_procs);  // How many processes total?

if (rank == 0) {
    // I'm the master!
    // Manage task queue and distribute work
} else {
    // I'm a worker!
    // Execute tasks assigned to me
}
```

**Communication:**
```cpp
// Non-blocking send (master â†’ worker)
MPI_Isend(&task, 1, TASK_T_TYPE, worker_rank, 0, 
          MPI_COMM_WORLD, &send_request);

// Non-blocking receive (worker â†’ master)
MPI_Irecv(desc_tasks[proc], Nmax, TASK_T_TYPE, worker_rank, 0,
          MPI_COMM_WORLD, &recv_request);

// Check if any messages arrived
MPI_Waitsome(num_requests, requests, &outcount, indices, 
             MPI_STATUSES_IGNORE);
```

**Why non-blocking?**
- Master doesn't wait for workers to finish before sending more tasks
- Workers can receive new tasks while still processing
- Maximizes throughput

### Level 2: OpenMP (Thread-Level Parallelism)

**What it does:**
- Creates multiple threads within a single process
- All threads share the same memory
- Much faster communication than MPI (shared memory)

**In our code:**
```cpp
#ifdef _OPENMP
    // Set 2 threads per MPI process
    omp_set_num_threads(2);
#endif

// When a worker executes a task, OpenMP can parallelize
// the computation across its threads
```

**Why use it?**
- Each worker (MPI process) has multiple CPU cores available
- OpenMP uses those cores to speed up individual task execution
- Example: If matrix multiplication takes 10s on 1 core, 
  it might take 5s on 2 cores with OpenMP

**Hybrid Model:**
```
Machine 1:                    Machine 2:
  MPI Process 0 (Master)        MPI Process 2 (Worker)
    [No OpenMP threads]           Thread 0
                                  Thread 1
                                  
  MPI Process 1 (Worker)        MPI Process 3 (Worker)
    Thread 0                      Thread 0
    Thread 1                      Thread 1
```

### Level 3: SIMD (Data-Level Parallelism)

**What it does:**
- Single CPU instruction processes multiple data elements
- Uses special wide registers (256-bit, 512-bit)
- Like having multiple ALUs (Arithmetic Logic Units) in one core

**In our code (Matrix Multiplication):**
```cpp
// WITHOUT SIMD:
for (int i = 0; i < n; i++) {
    for (int k = 0; k < p; k++) {
        float a_ik = A[{i, k}];
        for (int j = 0; j < p; j++) {
            C[{i, j}] += a_ik * B[{k, j}];
            // Each iteration: 1 multiply, 1 add
            // Processed ONE AT A TIME
        }
    }
}

// WITH SIMD:
for (int i = 0; i < n; i++) {
    for (int k = 0; k < p; k++) {
        float a_ik = A[{i, k}];
        #pragma omp simd  // â† Magic happens here!
        for (int j = 0; j < p; j++) {
            C[{i, j}] += a_ik * B[{k, j}];
            // If AVX2: processes 8 elements simultaneously
            // 8x faster on the inner loop!
        }
    }
}
```

**Visual representation:**
```
Traditional (Scalar):
  Register: [   value   ]  â† 64 bits, 1 float
  One operation per cycle

SIMD (AVX2):
  Register: [v0|v1|v2|v3|v4|v5|v6|v7]  â† 256 bits, 8 floats
  Eight operations per cycle
```

**What the compiler does:**
```
// Your code:
#pragma omp simd
for (j = 0; j < 8; j++) {
    C[j] += a * B[j];
}

// Compiler generates:
__m256 vec_a = _mm256_set1_ps(a);      // Broadcast 'a' to all 8 lanes
__m256 vec_B = _mm256_load_ps(&B[0]);  // Load 8 elements from B
__m256 vec_C = _mm256_load_ps(&C[0]);  // Load 8 elements from C
vec_C = _mm256_fmadd_ps(vec_a, vec_B, vec_C);  // C += a * B (8 at once!)
_mm256_store_ps(&C[0], vec_C);         // Store result back
```

---

## ğŸ’» Code Walkthrough

### File Structure

```
main.cpp          â† Entry point, MPI initialization
runner.cpp        â† Master-worker logic (THE BRAIN)
runner.hpp        â† Function declarations
tasks.cpp         â† Task execution kernels (THE MUSCLE)
tasks.hpp         â† Task definitions and constants
Makefile          â† Build system (creates 3 variants)
```

### Main Flow (`main.cpp`)

```cpp
int main(int argc, char *argv[]) {
    // 1. Initialize MPI
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);
    
    // 2. Create MPI datatypes for our custom structs
    MPI_Type_create_struct(..., &MPI_PARAMS_T);
    MPI_Type_create_struct(..., &MPI_METRIC_T);
    
    // 3. Master reads parameters from command line
    if (rank == 0) {
        params = parse_params(argv);  // H, Nmin, Nmax, P, input_file
    }
    
    // 4. Broadcast parameters to all processes
    MPI_Bcast(&params, 1, MPI_PARAMS_T, 0, MPI_COMM_WORLD);
    
    // 5. Start timing
    clock_gettime(CLOCK_MONOTONIC, &start);
    
    // 6. THE MAIN WORK HAPPENS HERE
    run_all_tasks(rank, num_procs, stats, params);
    
    // 7. Stop timing
    clock_gettime(CLOCK_MONOTONIC, &end);
    
    // 8. Gather statistics from all processes
    MPI_Gather(&stats, 1, MPI_METRIC_T, all_stats, ...);
    
    // 9. Master prints results
    if (rank == 0) {
        print_combined_metrics(all_stats);
        printf("FINAL RUNTIME: %ldms\n", elapsed);
    }
    
    // 10. Cleanup
    MPI_Finalize();
    return 0;
}
```

### Master Logic (`runner.cpp` - Master Branch)

```cpp
if (rank == MASTER) {
    // Initialize data structures
    queue<task_t> task_queue = get_initial_tasks(params);
    vector<bool> availability(N_WORKERS, true);
    vector<MPI_Request> proc_requests(N_WORKERS, MPI_REQUEST_NULL);
    
    while (true) {
        // 1. Check which workers finished and sent back results
        vector<int> worker_procs;  // List of available workers
        
        for (int proc = 0; proc < N_WORKERS; proc++) {
            if (availability[proc]) {
                worker_procs.push_back(proc);
                continue;  // Already available
            }
            
            // Check if this worker sent us results
            int flag;
            MPI_Test(&proc_requests[proc], &flag, MPI_STATUS_IGNORE);
            if (flag) {
                // Worker finished! Process returned tasks
                for (int i = 0; i < Nmax; i++) {
                    if (desc_tasks[proc][i].id != -1) {
                        task_queue.push(desc_tasks[proc][i]);
                    }
                }
                availability[proc] = true;
                worker_procs.push_back(proc);
            }
        }
        
        // 2. Are we done?
        if (task_queue.empty() && worker_procs.size() == N_WORKERS) {
            // All tasks done, all workers idle â†’ TERMINATE
            send_termination_signals();
            break;
        }
        
        // 3. Distribute tasks to available workers
        for (int proc : worker_procs) {
            if (!task_queue.empty()) {
                task_t task = task_queue.front();
                task_queue.pop();
                
                // Send task to worker
                int worker_rank = proc + 1;
                MPI_Isend(&task, 1, TASK_T_TYPE, worker_rank, 0, 
                          MPI_COMM_WORLD, &send_request);
                
                // Setup receive for worker's results
                desc_tasks[proc] = new task_t[Nmax];
                MPI_Irecv(desc_tasks[proc], Nmax, TASK_T_TYPE, 
                          worker_rank, 0, MPI_COMM_WORLD, 
                          &proc_requests[proc]);
                
                availability[proc] = false;
            }
        }
    }
}
```

**Key Points:**
- Master never executes tasks (rank 0 does coordination only)
- Uses non-blocking sends/receives (MPI_Isend, MPI_Irecv)
- Polls workers with MPI_Test (doesn't block waiting)
- Dynamic load balancing: fast workers get more tasks automatically

### Worker Logic (`runner.cpp` - Worker Branch)

```cpp
else {  // Worker
    #ifdef _OPENMP
        omp_set_num_threads(2);  // Use 2 threads per worker
    #endif
    
    while (true) {
        // 1. Receive task from master
        task_t task;
        MPI_Recv(&task, 1, TASK_T_TYPE, MASTER, 0, 
                 MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        
        // 2. Check for termination signal
        int signal;
        MPI_Recv(&signal, 1, MPI_INT, MASTER, TERM_TAG, 
                 MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        if (signal == SIG_TERM) {
            break;  // Shutdown
        }
        
        // 3. Execute the task
        int num_new_tasks = 0;
        vector<task_t> task_buffer(Nmax);
        
        execute_task(stats, task, num_new_tasks, task_buffer);
        
        // 4. Send results back to master
        if (num_new_tasks == 0) {
            // No children spawned, send invalid tasks
            fill(task_buffer.begin(), task_buffer.end(), INVALID_TASK);
        }
        
        MPI_Send(task_buffer.data(), Nmax, TASK_T_TYPE, 
                 MASTER, 0, MPI_COMM_WORLD);
    }
}
```

**Key Points:**
- Workers execute tasks using `execute_task()`
- Can spawn 0-N child tasks (stored in task_buffer)
- Always send back Nmax tasks (padded with INVALID_TASK if needed)
- Blocks waiting for next task (MPI_Recv)

### Task Execution (`tasks.cpp`)

```cpp
void execute_task(metric_t &stats, const task_t &task, 
                  int &num_new_tasks, vector<task_t> &task_buffer) {
    
    // 1. Execute the appropriate kernel based on task type
    switch (task.type) {
        case TaskType::PRIME:
            result = is_prime(n);  // Check primality
            break;
        case TaskType::MATMULT:
            result = matrix_multiply(n);  // SIMD optimized
            break;
        case TaskType::LCS:
            result = longest_common_subsequence(len, alph);
            break;
        case TaskType::SHA:
            hash = SHA(len, seed);  // Compute SHA-256
            break;
        case TaskType::BITONIC:
            bitonic_sort(array, 0, size, 1);  // SIMD optimized
            break;
    }
    
    // 2. Update statistics
    stats.completed[task.type]++;
    
    // 3. Maybe generate child tasks
    num_new_tasks = 0;
    if (task.gen < H) {  // Not at max depth yet
        float random = get_random_float();
        if (random < P) {  // Probability check
            int n = random_between(Nmin, Nmax);
            for (int i = 0; i < n; i++) {
                task_t child;
                child.gen = task.gen + 1;
                child.type = random_task_type();
                child.arg_seed = random_seed();
                child.id = generate_unique_id();
                task_buffer[num_new_tasks++] = child;
            }
        }
    }
}
```

### Matrix Multiplication (SIMD Optimized)

```cpp
uint32_t matrix_multiply(int n, int p, uint32_t seed) {
    matrix<float> A(n, p), B(p, p), C(n, p);
    
    // Initialize matrices with random values
    for (int i = 0; i < n * p; i++) {
        A._data[i] = random_value(seed);
    }
    for (int i = 0; i < p * p; i++) {
        B._data[i] = random_value(seed);
    }
    
    // Matrix multiplication: C = A Ã— B
    // Using ikj loop order (cache-friendly!)
    for (int i = 0; i < n; i++) {
        for (int k = 0; k < p; k++) {
            float a_ik = A[{i, k}];
            
            #ifdef USE_SIMD
            #pragma omp simd  // â† Vectorize this loop!
            #endif
            for (int j = 0; j < p; j++) {
                C[{i, j}] += a_ik * B[{k, j}];
            }
        }
    }
    
    // Return checksum
    uint32_t result = 0;
    for (int i = 0; i < n * p; i++) {
        result ^= (uint32_t)C._data[i];
    }
    return result;
}
```

**Why ikj order?**
```
Standard ijk:
  for i:
    for j:
      for k:
        C[i,j] += A[i,k] * B[k,j]
        
  Access pattern for B: B[0,j], B[1,j], B[2,j], ... (stride = N)
  BAD for cache! Jumps around memory

Optimized ikj:
  for i:
    for k:
      a = A[i,k]
      for j:
        C[i,j] += a * B[k,j]
        
  Access pattern for B: B[k,0], B[k,1], B[k,2], ... (stride = 1)
  GOOD for cache! Sequential memory access
  SIMD friendly! Can load 8 consecutive elements
```

---

## ğŸ“Š Performance Analysis

### What Gets Measured

```cpp
struct metric_t {
    int rank;                      // Which process
    array<int, 5> completed;       // Tasks completed by type
    int busy;                      // 1 if busy, 0 if idle
    int64_t elapsed;               // Time spent working (ms)
};
```

### Output Explanation

```
========================== EXECUTION METRICS ==========================
Rank 0: 0ms of 27972ms (0.00000) - completed: 0 0 0 0 0
Rank 1: 27749ms of 27972ms (0.99203) - completed: 24 19 15 24 24
Rank 2: 27906ms of 27972ms (0.99764) - completed: 28 27 20 28 30
Rank 3: 27678ms of 27972ms (0.98949) - completed: 16 16 17 15 12
Overall: 83333ms of 111888ms (0.74479) - completed: 68 62 52 67 66
FINAL RUNTIME: 27972ms
```

**Reading this:**
- **Rank 0**: Master (0ms busy, does no computation)
- **Rank 1**: Worker 1 was busy 27749ms out of 27972ms (99.2% utilization)
  - Completed: 24 PRIME, 19 MATMULT, 15 LCS, 24 SHA, 24 BITONIC
- **Overall CPU time**: 83333ms (sum of all workers)
- **Wall clock time**: 27972ms (actual elapsed time)
- **Parallelization efficiency**: 83333 / (27972 Ã— 3) = 99.2%

### Speedup Calculation

```
Speedup = Sequential Time / Parallel Time

Example:
  Sequential (1 core):     120 seconds
  MPI-only (4 processes):   35 seconds  â†’ 3.4x speedup
  MPI+OpenMP (4Ã—2):         20 seconds  â†’ 6.0x speedup
  MPI+OpenMP+SIMD (4Ã—2):    15 seconds  â†’ 8.0x speedup
```

**Ideal vs Actual:**
- Ideal with 4 processes: 4x speedup
- Actual: ~3.4x (communication overhead, load imbalance)
- With OpenMP: ~6x (not 8x due to Amdahl's Law)
- With SIMD: ~8x (depends on vectorizable code percentage)

### Why Not Perfect Scaling?

**Amdahl's Law:**
```
If 90% of code is parallelizable:
  Max speedup with âˆ processors = 1 / (1 - 0.90) = 10x

If 95% of code is parallelizable:
  Max speedup with âˆ processors = 1 / (1 - 0.95) = 20x
```

**Sources of overhead:**
1. **Communication**: Sending tasks and results takes time
2. **Load imbalance**: Some tasks take longer than others
3. **Master bottleneck**: Master must coordinate everyone
4. **Synchronization**: Waiting for all workers to finish

---

## ğŸ“ Key Takeaways for Your Presentation

### 1. Problem Statement
"Modern applications need to process massive amounts of data. Single-core performance has plateaued, so we need to use multiple levels of parallelism to achieve high performance."

### 2. Solution
"We implemented a three-level parallel task scheduler:
- **MPI** for distributed computing across machines
- **OpenMP** for multi-threading within machines  
- **SIMD** for vectorization within cores"

### 3. Architecture
"Master-worker pattern with dynamic load balancing. The master coordinates work, workers execute tasks using all available parallelism."

### 4. Results
"Achieved X.Xx speedup over sequential execution by combining all three levels of parallelism. Each level contributes to the overall performance gain."

### 5. Real-World Applications
- Video processing pipelines (Netflix, YouTube)
- Scientific simulations (weather, molecular dynamics)
- Machine learning training (distributed gradient descent)
- Cryptocurrency mining (parallel hash computation)
- Database query execution (parallel scans and joins)

---

## ğŸ¤” Common Questions & Answers

**Q: Why not just use a single level of parallelism?**
A: Different levels target different hardware resources. MPI uses multiple machines, OpenMP uses multiple cores within a machine, and SIMD uses vector units within a core. Using all three maximizes hardware utilization.

**Q: When would you use only MPI?**
A: When tasks are very large and can run independently for a long time. Communication overhead becomes negligible compared to computation time.

**Q: When would you use only OpenMP?**
A: When you have a single machine with many cores and tasks need to share a lot of data. Shared memory is much faster than message passing.

**Q: What's the hardest part of this project?**
A: Dynamic load balancing and non-blocking communication. The master must track multiple workers simultaneously, handle tasks arriving at unpredictable times, and avoid deadlocks.

**Q: Can this scale to thousands of machines?**
A: The master-worker pattern becomes a bottleneck at large scale. For thousands of machines, you'd need hierarchical masters or fully distributed coordination (like MapReduce).

**Q: How does this compare to GPU programming?**
A: GPUs are another level of parallelism (thousands of simple cores). This project focuses on CPU parallelism. In practice, you might combine both - use MPI+OpenMP for the framework and GPUs for compute kernels.

---

## ğŸ¯ Summary

This project demonstrates **hierarchical parallelism** - using multiple levels of parallel computing to maximize performance on modern hardware:

1. **MPI**: Distributed processes (across machines)
2. **OpenMP**: Shared-memory threads (across cores)
3. **SIMD**: Vector operations (within cores)

The **master-worker architecture** provides dynamic load balancing, and **non-blocking communication** ensures high throughput. By combining all three levels, we achieve significantly better performance than any single level alone.

The skills demonstrated here are fundamental to modern high-performance computing and are used in real-world systems ranging from web services to scientific computing to machine learning.

---

**Now you understand the project deeply! You can explain it confidently to your jury.** ğŸ“âœ¨
